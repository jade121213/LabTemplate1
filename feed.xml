<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://jade121213.github.io/LabTemplate1/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jade121213.github.io/LabTemplate1/" rel="alternate" type="text/html" /><updated>2025-04-27T21:07:29+00:00</updated><id>https://jade121213.github.io/LabTemplate1/feed.xml</id><title type="html">jade121213</title><subtitle>An engaging 1-3 sentence description of your lab.</subtitle><entry><title type="html">三维重建以及神经渲染中的学习</title><link href="https://jade121213.github.io/LabTemplate1/2023/02/23/example-post-3.html" rel="alternate" type="text/html" title="三维重建以及神经渲染中的学习" /><published>2023-02-23T00:00:00+00:00</published><updated>2025-04-27T21:05:22+00:00</updated><id>https://jade121213.github.io/LabTemplate1/2023/02/23/example-post-3</id><content type="html" xml:base="https://jade121213.github.io/LabTemplate1/2023/02/23/example-post-3.html"><![CDATA[<p>1：摘要
提出基于学习（learning-based）方法，使用野外照片的非结构化集合（unstructured collections of in-the-wild photographs）来合成复杂场景。之前的Nerf通过MLP的权重来模拟场景的密度、颜色。虽然在静态对象生成上比较好，但在uncontrolled images不受控的图片中，会有一些ubiquitous，real-world phenomenon，也就是可变照明或者瞬时遮光器variable illumination or transient occluders，本文基于Nerf引入了一些列扩展来解决这些问题。</p>

<p>概况来讲：
Nerf要求在相同位置、视角拍摄的照片完全一样，也就是必须在尽可能短的时间内拍的照片，因为这种情况下光线变化等影响会很小；nerf-wild放松了限制，通过解决光照变化以及移动遮挡的问题，来使得输入的照片不一定完全一样，同一个位置、同一个视角上午拍的或者下午拍的都可以作为输入。</p>]]></content><author><name>john-doe</name></author><category term="biology," /><category term="medicine" /><summary type="html"><![CDATA[1：摘要 提出基于学习（learning-based）方法，使用野外照片的非结构化集合（unstructured collections of in-the-wild photographs）来合成复杂场景。之前的Nerf通过MLP的权重来模拟场景的密度、颜色。虽然在静态对象生成上比较好，但在uncontrolled images不受控的图片中，会有一些ubiquitous，real-world phenomenon，也就是可变照明或者瞬时遮光器variable illumination or transient occluders，本文基于Nerf引入了一些列扩展来解决这些问题。]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://jade121213.github.io/LabTemplate1/images/photo.jpg" /><media:content medium="image" url="https://jade121213.github.io/LabTemplate1/images/photo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">深度学习入门教程（2）：使用预训练模型来文字生成图片TextToImageGenerationWithNetwork</title><link href="https://jade121213.github.io/LabTemplate1/2021/09/30/example-post-2.html" rel="alternate" type="text/html" title="深度学习入门教程（2）：使用预训练模型来文字生成图片TextToImageGenerationWithNetwork" /><published>2021-09-30T00:00:00+00:00</published><updated>2025-04-27T21:05:22+00:00</updated><id>https://jade121213.github.io/LabTemplate1/2021/09/30/example-post-2</id><content type="html" xml:base="https://jade121213.github.io/LabTemplate1/2021/09/30/example-post-2.html"><![CDATA[<p>1：什么是文本到图像生成？
文本到图像模型是一种机器学习模型，它将自然语言描述作为输入并生成与该描述匹配的图像。 由于深度神经网络的进步，此类模型于 2010 年代中期开始开发。 2022 年，最先进的文本到图像模型的输出，例如 OpenAI 的 DALL-E 2、Google Brain 的 Imagen、Midjourney 和 StabilityAI 的 Stable Diffusion 的输出开始接近真实照片和人类绘制艺术的质量。</p>

<p>文本到图像模型通常结合了语言模型和生成图像模型，其中语言模型将输入文本转换为潜在表示，生成图像模型生成以该表示为条件的图像。 最有效的模型通常是根据抓取的大量图像和文本数据进行训练的来自网络。</p>]]></content><author><name>john</name></author><summary type="html"><![CDATA[1：什么是文本到图像生成？ 文本到图像模型是一种机器学习模型，它将自然语言描述作为输入并生成与该描述匹配的图像。 由于深度神经网络的进步，此类模型于 2010 年代中期开始开发。 2022 年，最先进的文本到图像模型的输出，例如 OpenAI 的 DALL-E 2、Google Brain 的 Imagen、Midjourney 和 StabilityAI 的 Stable Diffusion 的输出开始接近真实照片和人类绘制艺术的质量。]]></summary></entry><entry><title type="html">Neus：Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</title><link href="https://jade121213.github.io/LabTemplate1/2019/01/07/example-post-1.html" rel="alternate" type="text/html" title="Neus：Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction" /><published>2019-01-07T00:00:00+00:00</published><updated>2025-04-27T21:05:22+00:00</updated><id>https://jade121213.github.io/LabTemplate1/2019/01/07/example-post-1</id><content type="html" xml:base="https://jade121213.github.io/LabTemplate1/2019/01/07/example-post-1.html"><![CDATA[<p>摘要本文的总目标是实现从2D图片到3D模型的高保真重建（使用神经渲染方法）。2020年Niemeyer等人提出的DVR和2020年Yariv等人提出的IDR是现存的神经表面重建（neural surface reconstruction）方法，但他们都依赖前景遮罩（foreground mask）提供监督且训练不稳定。近期的神经方法比如2020年的NeRF及其变体提出使用体积渲染（volume rendering）构造更鲁棒的场景描述，但由于缺少表面约束（surface constraints）在这种隐式（implicit）场景描述的基础上构造表面十分困难。于是本文提出一种新的神经表面重建方法NeuS，使用一个符号距离函数（SDF，signed distance function）的零级集合（zero-level set）来表示一个表面，并部署一种新的体积渲染方法来训练神经SDF描述。作者发现传统体积渲染会在表面重建引入固有几何错误比如bias，所以引入了新方法，改进这一情况，即使没有遮罩监督。</p>]]></content><author><name>sarah-johnson</name></author><category term="biology" /><category term="medicine" /><category term="big data" /><summary type="html"><![CDATA[摘要本文的总目标是实现从2D图片到3D模型的高保真重建（使用神经渲染方法）。2020年Niemeyer等人提出的DVR和2020年Yariv等人提出的IDR是现存的神经表面重建（neural surface reconstruction）方法，但他们都依赖前景遮罩（foreground mask）提供监督且训练不稳定。近期的神经方法比如2020年的NeRF及其变体提出使用体积渲染（volume rendering）构造更鲁棒的场景描述，但由于缺少表面约束（surface constraints）在这种隐式（implicit）场景描述的基础上构造表面十分困难。于是本文提出一种新的神经表面重建方法NeuS，使用一个符号距离函数（SDF，signed distance function）的零级集合（zero-level set）来表示一个表面，并部署一种新的体积渲染方法来训练神经SDF描述。作者发现传统体积渲染会在表面重建引入固有几何错误比如bias，所以引入了新方法，改进这一情况，即使没有遮罩监督。]]></summary></entry></feed>